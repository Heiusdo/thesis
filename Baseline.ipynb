{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fbb9f5",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb462504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "182d8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load data\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "valid = pd.read_csv(\"data/valid.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# 2) Pick columns (handle missing)\n",
    "X_train = train[\"text_clean\"]\n",
    "y_train = train[\"label\"]\n",
    "X_valid = valid[\"text_clean\"]\n",
    "y_valid = valid[\"label\"]\n",
    "X_test = test[\"text_clean\"]\n",
    "y_test = test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac69d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Pipeline \n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        strip_accents=\"unicode\"\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        penalty=\"l2\",\n",
    "        max_iter=3000,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\"\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5b55c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Param grid \n",
    "param_grid = {\n",
    "    \"tfidf__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"tfidf__min_df\": [1, 2, 5],\n",
    "    \"tfidf__max_df\": [0.9, 0.95, 1.0],\n",
    "    \"tfidf__sublinear_tf\": [True, False],\n",
    "    \"clf__C\": [0.25, 0.5, 1.0, 2.0, 4.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "711e0b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n",
      "\n",
      "Best CV score (f1_macro): 0.6756785211529684\n",
      "Best params: {'clf__C': 0.5, 'tfidf__max_df': 0.9, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2), 'tfidf__sublinear_tf': True}\n"
     ]
    }
   ],
   "source": [
    "# 5) GridSearchCV \n",
    "# Use macro-F1 for multi-class; change to \"f1_weighted\" if your classes are imbalanced and you care about overall\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest CV score (f1_macro):\", grid.best_score_)\n",
    "print(\"Best params:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ee5c2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Holdout Accuracy: 0.6933997509339975\n",
      "Holdout Macro F1: 0.695131974274536\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.69      1188\n",
      "           1       0.63      0.62      0.63      1472\n",
      "           2       0.77      0.76      0.77      1355\n",
      "\n",
      "    accuracy                           0.69      4015\n",
      "   macro avg       0.69      0.70      0.70      4015\n",
      "weighted avg       0.69      0.69      0.69      4015\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 838  282   68]\n",
      " [ 323  910  239]\n",
      " [  73  246 1036]]\n"
     ]
    }
   ],
   "source": [
    "# 6) Evaluate on valid set \n",
    "best_model = grid.best_estimator_\n",
    "pred = best_model.predict(X_valid)\n",
    "\n",
    "print(\"\\nHoldout Accuracy:\", accuracy_score(y_valid, pred))\n",
    "print(\"Holdout Macro F1:\", f1_score(y_valid, pred, average=\"macro\"))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_valid, pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_valid, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "206f73d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.6800298804780877\n",
      "Test Macro F1: 0.6819684140376904\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.67      0.68      1188\n",
      "           1       0.61      0.62      0.62      1473\n",
      "           2       0.76      0.75      0.75      1355\n",
      "\n",
      "    accuracy                           0.68      4016\n",
      "   macro avg       0.68      0.68      0.68      4016\n",
      "weighted avg       0.68      0.68      0.68      4016\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 801  311   76]\n",
      " [ 309  914  250]\n",
      " [  72  267 1016]]\n"
     ]
    }
   ],
   "source": [
    "# 7) Evaluate on test set \n",
    "best_model = grid.best_estimator_\n",
    "pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"Test Macro F1:\", f1_score(y_test, pred, average=\"macro\"))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c2cb7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6861768368617683\n",
      "Macro F1: 0.6879467874219563\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.64      0.67      1188\n",
      "           1       0.61      0.65      0.63      1472\n",
      "           2       0.77      0.76      0.76      1355\n",
      "\n",
      "    accuracy                           0.69      4015\n",
      "   macro avg       0.69      0.69      0.69      4015\n",
      "weighted avg       0.69      0.69      0.69      4015\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 765  353   70]\n",
      " [ 272  959  241]\n",
      " [  55  269 1031]]\n",
      "Saved to logreg_tfidf.joblib\n"
     ]
    }
   ],
   "source": [
    "# 1) Load data\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "valid = pd.read_csv(\"data/valid.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# 2) Pick columns (handle missing)\n",
    "X_train = train[\"text_clean\"]\n",
    "y_train = train[\"label\"]\n",
    "X_valid = valid[\"text_clean\"]\n",
    "y_valid = valid[\"label\"]\n",
    "X_test = test[\"text_clean\"]\n",
    "y_test = test[\"label\"]\n",
    "\n",
    "# 4) Pipeline: TF-IDF -> Logistic Regression (multiclass)\n",
    "model = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        strip_accents=\"unicode\",\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        sublinear_tf=True\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        n_jobs=-1,\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 5) Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 6) Evaluate\n",
    "pred = model.predict(X_valid)\n",
    "print(\"Accuracy:\", accuracy_score(y_valid, pred))\n",
    "print(\"Macro F1:\", f1_score(y_valid, pred, average=\"macro\"))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_valid, pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_valid, pred))\n",
    "\n",
    "# 7) Save model\n",
    "joblib.dump(model, \"logreg_tfidf.joblib\")\n",
    "print(\"Saved to logreg_tfidf.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48373e",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6c74c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n",
      "Best CV Macro-F1: 0.6758208266951533\n",
      "Best params: {'clf__C': 0.25, 'tfidf__max_df': 0.9, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 2), 'tfidf__sublinear_tf': True}\n",
      "Valid Accuracy: 0.687173100871731\n",
      "Valid Macro F1: 0.6863049492832762\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.71      0.69      1188\n",
      "           1       0.65      0.56      0.60      1472\n",
      "           2       0.74      0.80      0.77      1355\n",
      "\n",
      "    accuracy                           0.69      4015\n",
      "   macro avg       0.68      0.69      0.69      4015\n",
      "weighted avg       0.68      0.69      0.68      4015\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 840  259   89]\n",
      " [ 344  831  297]\n",
      " [  80  187 1088]]\n",
      "Test Accuracy: 0.6795318725099602\n",
      "Test Macro F1: 0.6794786076149442\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69      1188\n",
      "           1       0.64      0.57      0.60      1473\n",
      "           2       0.72      0.79      0.75      1355\n",
      "\n",
      "    accuracy                           0.68      4016\n",
      "   macro avg       0.68      0.68      0.68      4016\n",
      "weighted avg       0.68      0.68      0.68      4016\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 830  258  100]\n",
      " [ 319  834  320]\n",
      " [  72  218 1065]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# =========================\n",
    "# Load data\n",
    "# =========================\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "valid = pd.read_csv(\"data/valid.csv\")\n",
    "test  = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "X_train = train[\"text_clean\"]\n",
    "y_train = train[\"label\"]\n",
    "X_valid = valid[\"text_clean\"]\n",
    "y_valid = valid[\"label\"]\n",
    "X_test  = test[\"text_clean\"]\n",
    "y_test  = test[\"label\"]\n",
    "\n",
    "# =========================\n",
    "# Pipeline: TF-IDF + Linear SVM\n",
    "# =========================\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        strip_accents=\"unicode\"\n",
    "    )),\n",
    "    (\"clf\", LinearSVC(\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=5000\n",
    "    ))\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# Param grid (song song vá»›i LR)\n",
    "# =========================\n",
    "param_grid = {\n",
    "    \"tfidf__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"tfidf__min_df\": [1, 2, 5],\n",
    "    \"tfidf__max_df\": [0.9, 0.95, 1.0],\n",
    "    \"tfidf__sublinear_tf\": [True, False],\n",
    "    \"clf__C\": [0.25, 0.5, 1.0, 2.0, 4.0],\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# GridSearch\n",
    "# =========================\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CV Macro-F1:\", grid.best_score_)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "\n",
    "# =========================\n",
    "# Evaluate on valid\n",
    "# =========================\n",
    "best_model = grid.best_estimator_\n",
    "pred = best_model.predict(X_valid)\n",
    "\n",
    "print(\"Valid Accuracy:\", accuracy_score(y_valid, pred))\n",
    "print(\"Valid Macro F1:\", f1_score(y_valid, pred, average=\"macro\"))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_valid, pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_valid, pred))\n",
    "\n",
    "# =========================\n",
    "# Evaluate on test\n",
    "# =========================\n",
    "pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"Test Macro F1:\", f1_score(y_test, pred, average=\"macro\"))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e9feefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6547945205479452\n",
      "Macro F1: 0.6558016780969845\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.66      0.65      1188\n",
      "           1       0.58      0.55      0.57      1472\n",
      "           2       0.73      0.76      0.75      1355\n",
      "\n",
      "    accuracy                           0.65      4015\n",
      "   macro avg       0.65      0.66      0.66      4015\n",
      "weighted avg       0.65      0.65      0.65      4015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        strip_accents=\"unicode\",\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        sublinear_tf=True\n",
    "    )),\n",
    "    (\"clf\", LinearSVC(\n",
    "        max_iter=5000\n",
    "    ))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_valid)\n",
    "print(\"Accuracy:\", accuracy_score(y_valid, pred))\n",
    "print(\"Macro F1:\", f1_score(y_valid, pred, average=\"macro\"))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_valid, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b79048",
   "metadata": {},
   "source": [
    "# Roberta-Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "TEST_CSV_PATH = \"data/test.csv\"  \n",
    "task = \"sentiment\"\n",
    "MODEL_NAME = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e4e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   : 0.636203349985307\n",
      "Macro F1   : 0.6371563833433259\n",
      "Macro Prec : 0.659878647015379\n",
      "Macro Recall: 0.6313676681735534\n",
      "Macro ROC-AUC (OVR): 0.8192323538565706\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Dataset\n",
    "# =========================\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "def collate_fn(batch_texts, tokenizer):\n",
    "    return tokenizer(\n",
    "        batch_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# Load data\n",
    "# =========================\n",
    "df = pd.read_csv(TEST_CSV_PATH)\n",
    "X = df[\"text_clean\"].fillna(\"\").astype(str).tolist()\n",
    "y_true = df[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "# =========================\n",
    "# Load model/tokenizer\n",
    "# =========================\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# Inference (batched)\n",
    "# =========================\n",
    "loader = DataLoader(\n",
    "    TextDataset(X),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: collate_fn(x, tokenizer),\n",
    ")\n",
    "\n",
    "all_probs = []\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        logits = model(**batch).logits.detach().cpu().numpy()  # (B, 3)\n",
    "        probs = softmax(logits, axis=1)                        # (B, 3)\n",
    "        all_probs.append(probs)\n",
    "\n",
    "probs = np.vstack(all_probs)          # (N, 3)\n",
    "y_pred = probs.argmax(axis=1)         # (N,)\n",
    "\n",
    "# =========================\n",
    "# Metrics\n",
    "# =========================\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1  = f1_score(y_true, y_pred, average=\"macro\")\n",
    "prec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "rec  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "# ROC-AUC multi-class (OvR) needs probabilities\n",
    "\n",
    "try:\n",
    "    auc = roc_auc_score(\n",
    "        y_true,\n",
    "        probs,\n",
    "        multi_class=\"ovr\",\n",
    "        average=\"macro\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    auc = None\n",
    "    print(f\"[WARN] ROC-AUC cannot be computed: {e}\")\n",
    "\n",
    "print(\"Accuracy   :\", acc)\n",
    "print(\"Macro F1   :\", f1)\n",
    "print(\"Macro Prec :\", prec)\n",
    "print(\"Macro Recall:\", rec)\n",
    "print(\"Macro ROC-AUC (OVR):\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed13a86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
